{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total trainning batch number: 1875\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "mnist_data_path = './MNIST_data'\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = MNIST(root=mnist_data_path, train=True, transform=trans, download=True)\n",
    "test_set = MNIST(root=mnist_data_path, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=200,\n",
    "                shuffle=True)\n",
    "\n",
    "print('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_hierarchy(path):\n",
    "    tree_labels_path = {}\n",
    "    tree_label_full_path = {}\n",
    "    tree_paths = set()\n",
    "    p2t = {}\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            label, l_path = line.split(',')[:2]\n",
    "            full_path = l_path.strip()\n",
    "            p2t[full_path] = int(label)\n",
    "            tree_label_full_path[int(label)]=[int(l) for l in list(full_path)]\n",
    "            path_labels ={}\n",
    "            for k in range(1,1+len(full_path)):\n",
    "                tree_paths.add(full_path[:k-1])\n",
    "                path_labels[full_path[:k-1]] = int(full_path[k-1])\n",
    "            tree_labels_path[int(label)] = path_labels\n",
    "    path_inds = {k:i for i,k in enumerate(sorted(tree_paths,key=len))}\n",
    "    tree_labels_path_indexed = {\n",
    "        l:{path_inds[p]:p_l for p,p_l in path_dict.items()} \n",
    "        for l, path_dict in tree_labels_path.items()\n",
    "    }\n",
    "    labels_hier_idx = {}\n",
    "    for k, v in tree_labels_path_indexed.items():\n",
    "        idx,labs = list(zip(*v.items()))\n",
    "        labels_hier_idx[k] = (list(idx),list(labs))\n",
    "    return labels_hier_idx, len(tree_paths), path_inds, p2t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSoftmaxEnsemble(nn.Module):\n",
    "    def __init__(self, input_dim, trees_path, device=None):\n",
    "        super().__init__()\n",
    "        self.num_paths = None\n",
    "        self.path_indices = []\n",
    "        self.path2label =  []\n",
    "        self.num_hsfmx = 0\n",
    "        self.labels2path_labels = []\n",
    "        self.labels2path_labels_combined = {}\n",
    "        self.trees_path = trees_path\n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        self.read_trees(trees_path)\n",
    "        self.linear = nn.Linear(input_dim, self.num_paths * self.num_hsfmx)\n",
    "        self.linear.to(device)\n",
    "        \n",
    "    def read_trees(self, trees_path):\n",
    "        for path in glob.glob(trees_path):\n",
    "            labels_hier_idx, num_of_paths, path_idx, p2t = get_label_hierarchy(path)\n",
    "            labels_hier_idx = {\n",
    "                k:(torch.tensor(v[0]).long().to(device),\n",
    "                   torch.tensor(v[1]).float().to(device))\n",
    "                for k,v in labels_hier_idx.items()\n",
    "            }\n",
    "            self.labels2path_labels.append(labels_hier_idx)\n",
    "            if self.num_paths is None:\n",
    "                self.num_paths = num_of_paths\n",
    "            else:\n",
    "                assert self.num_paths == num_of_paths\n",
    "            self.path_indices.append(path_idx)\n",
    "            self.path2label.append(p2t)\n",
    "        self.num_hsfmx = len(self.labels2path_labels)\n",
    "        \n",
    "        self.labels2path_labels_combined = {}\n",
    "        \n",
    "        for k in range(len(self.labels2path_labels[0])):\n",
    "            comb_idx = torch.cat([\n",
    "                h_idx[k][0] + self.num_paths*m \n",
    "                for m, h_idx in enumerate(self.labels2path_labels)])\n",
    "            comb_labels = torch.cat([h_idx[k][1] for h_idx in self.labels2path_labels])\n",
    "            self.labels2path_labels_combined[k] = (comb_idx, comb_labels)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        for i in range(len(self.labels2path_labels)):\n",
    "            self.labels2path_labels[i] = {\n",
    "                k:(torch.tensor(v[0]).long().to(self.device),\n",
    "                   torch.tensor(v[1]).float().to(self.device))\n",
    "                for k, v in self.labels2path_labels[i].items()\n",
    "            }\n",
    "\n",
    "    def pred_label_single_hsfmx(self, pred, path_idx, p2t, start_ind=0):\n",
    "        current_node=0\n",
    "        current_path = []\n",
    "        cur_node_path_idx = [0]\n",
    "        while True:     \n",
    "            next_path_pred = pred[start_ind+cur_node_path_idx[-1]]\n",
    "            current_path.append('1' if next_path_pred.item() >= 0 else '0')\n",
    "            new_path = ''.join(current_path)\n",
    "            if new_path in p2t:\n",
    "                return p2t[new_path]\n",
    "            cur_node_path_idx.append(path_idx[new_path])\n",
    "\n",
    "    def get_labels(self, output, get_mode=True):\n",
    "        pred = torch.Tensor([\n",
    "            self.pred_label_single_hsfmx(row, path_idx, p2t, k * self.num_paths) \n",
    "            for row in output\n",
    "            for k, (path_idx, p2t) in enumerate(zip(self.path_indices, self.path2label))\n",
    "        ]).long().to(self.device).reshape(output.size(0), self.num_hsfmx)\n",
    "        return pred.mode(dim=1)[0] if get_mode else pred\n",
    "    \n",
    "    def forward(self, x, target=None, collect_paths=True, pred_labels=False, pred_labels__get_mode=True):\n",
    "        output =  self.linear(x)\n",
    "        if not collect_paths:\n",
    "            return output\n",
    "\n",
    "        y_hsfmx_idx = torch.cat([\n",
    "            row * self.num_paths * self.num_hsfmx + self.labels2path_labels_combined[l][0] \n",
    "            for row, l in enumerate(target.tolist())])\n",
    "        target_hsfmx = torch.cat([\n",
    "            self.labels2path_labels_combined[l][1] \n",
    "            for row, l in enumerate(target.tolist())])\n",
    "        output_hsfmx =  torch.gather(output.flatten(), 0, y_hsfmx_idx)\n",
    "        \n",
    "        if pred_labels:\n",
    "            return output_hsfmx, target_hsfmx, self.get_labels(output, pred_labels__get_mode)\n",
    "        return output_hsfmx, target_hsfmx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "model.fc = Identity()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 1, train loss: 0.771188\n",
      "==>>> epoch: 0, batch index: 101, train loss: 0.073097\n",
      "==>>> epoch: 0, batch index: 201, train loss: 0.064505\n",
      "==>>> epoch: 0, batch index: 301, train loss: 0.041280\n",
      "==>>> epoch: 0, batch index: 401, train loss: 0.050774\n",
      "==>>> epoch: 0, batch index: 501, train loss: 0.038083\n",
      "==>>> epoch: 0, batch index: 601, train loss: 0.032977\n",
      "==>>> epoch: 0, batch index: 701, train loss: 0.032610\n",
      "==>>> epoch: 0, batch index: 801, train loss: 0.033053\n",
      "==>>> epoch: 0, batch index: 901, train loss: 0.027311\n",
      "==>>> epoch: 0, batch index: 1001, train loss: 0.033389\n",
      "==>>> epoch: 0, batch index: 1101, train loss: 0.025873\n",
      "==>>> epoch: 0, batch index: 1201, train loss: 0.023599\n",
      "==>>> epoch: 0, batch index: 1301, train loss: 0.021803\n",
      "==>>> epoch: 0, batch index: 1401, train loss: 0.021761\n",
      "==>>> epoch: 0, batch index: 1501, train loss: 0.027197\n",
      "==>>> epoch: 0, batch index: 1601, train loss: 0.015227\n",
      "==>>> epoch: 0, batch index: 1701, train loss: 0.020455\n",
      "==>>> epoch: 0, batch index: 1801, train loss: 0.016001\n",
      "==>>> epoch: 0, batch index: 1875, train loss: 0.038813\n",
      "==>>> epoch: 0, batch index: 50, test loss: 0.023383, acc: 0.974\n",
      "==>>> epoch: 1, batch index: 1, train loss: 0.001941\n",
      "==>>> epoch: 1, batch index: 101, train loss: 0.027800\n",
      "==>>> epoch: 1, batch index: 201, train loss: 0.019159\n",
      "==>>> epoch: 1, batch index: 301, train loss: 0.025232\n",
      "==>>> epoch: 1, batch index: 401, train loss: 0.013914\n",
      "==>>> epoch: 1, batch index: 501, train loss: 0.025606\n",
      "==>>> epoch: 1, batch index: 601, train loss: 0.013016\n",
      "==>>> epoch: 1, batch index: 701, train loss: 0.020076\n",
      "==>>> epoch: 1, batch index: 801, train loss: 0.021800\n",
      "==>>> epoch: 1, batch index: 901, train loss: 0.014935\n",
      "==>>> epoch: 1, batch index: 1001, train loss: 0.016573\n",
      "==>>> epoch: 1, batch index: 1101, train loss: 0.027103\n",
      "==>>> epoch: 1, batch index: 1201, train loss: 0.016934\n",
      "==>>> epoch: 1, batch index: 1301, train loss: 0.010630\n",
      "==>>> epoch: 1, batch index: 1401, train loss: 0.013937\n",
      "==>>> epoch: 1, batch index: 1501, train loss: 0.019883\n",
      "==>>> epoch: 1, batch index: 1601, train loss: 0.009721\n",
      "==>>> epoch: 1, batch index: 1701, train loss: 0.007000\n",
      "==>>> epoch: 1, batch index: 1801, train loss: 0.015795\n",
      "==>>> epoch: 1, batch index: 1875, train loss: 0.019289\n",
      "==>>> epoch: 1, batch index: 50, test loss: 0.009552, acc: 0.991\n",
      "==>>> epoch: 2, batch index: 1, train loss: 0.002161\n",
      "==>>> epoch: 2, batch index: 101, train loss: 0.010765\n",
      "==>>> epoch: 2, batch index: 201, train loss: 0.017856\n",
      "==>>> epoch: 2, batch index: 301, train loss: 0.016510\n",
      "==>>> epoch: 2, batch index: 401, train loss: 0.012202\n",
      "==>>> epoch: 2, batch index: 501, train loss: 0.014759\n",
      "==>>> epoch: 2, batch index: 601, train loss: 0.015028\n",
      "==>>> epoch: 2, batch index: 701, train loss: 0.009592\n",
      "==>>> epoch: 2, batch index: 801, train loss: 0.009663\n",
      "==>>> epoch: 2, batch index: 901, train loss: 0.019150\n",
      "==>>> epoch: 2, batch index: 1001, train loss: 0.023375\n",
      "==>>> epoch: 2, batch index: 1101, train loss: 0.024454\n",
      "==>>> epoch: 2, batch index: 1201, train loss: 0.009245\n",
      "==>>> epoch: 2, batch index: 1301, train loss: 0.014043\n",
      "==>>> epoch: 2, batch index: 1401, train loss: 0.014231\n",
      "==>>> epoch: 2, batch index: 1501, train loss: 0.017345\n",
      "==>>> epoch: 2, batch index: 1601, train loss: 0.008054\n",
      "==>>> epoch: 2, batch index: 1701, train loss: 0.017154\n",
      "==>>> epoch: 2, batch index: 1801, train loss: 0.018109\n",
      "==>>> epoch: 2, batch index: 1875, train loss: 0.008807\n",
      "==>>> epoch: 2, batch index: 50, test loss: 0.012199, acc: 0.989\n"
     ]
    }
   ],
   "source": [
    "mnist_hsfmx = HierarchicalSoftmaxEnsemble(input_dim=512, trees_path='clusters/*', device=device)\n",
    "optimizer = optim.Adam(list(model.parameters()) + list(mnist_hsfmx.parameters()), lr=0.001)\n",
    "criterion_hsmx = nn.BCEWithLogitsLoss()\n",
    "for epoch in range(3):\n",
    "    # trainning\n",
    "    ave_loss = None\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        x = torch.cat((x,x,x), dim=1)\n",
    "        target = target.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        output_fc = model(x)\n",
    "        output_hsfmx, target_hsfmx = mnist_hsfmx(output_fc, target)\n",
    "        loss  = criterion_hsmx(output_hsfmx, target_hsfmx)\n",
    "\n",
    "        ave_loss = loss.item() if ave_loss is None else ave_loss * 0.9 + loss.item() * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss))\n",
    "            \n",
    "    # testing\n",
    "    correct_cnt, ave_loss_val = 0, None\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        x = torch.cat((x,x,x), dim=1)\n",
    "        out = model(x)\n",
    "        output_fc = model(x)\n",
    "        output_hsfmx, target_hsfmx, preds = mnist_hsfmx(output_fc, target, pred_labels=True)\n",
    "        loss  = criterion_hsmx(output_hsfmx, target_hsfmx)\n",
    "\n",
    "        total_cnt += x.size(0)\n",
    "        correct_cnt += (preds == target).sum()\n",
    "        # smooth average\n",
    "        ave_loss_val = loss.item() if ave_loss_val is None else ave_loss_val * 0.9 + loss.item() * 0.1\n",
    "\n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss_val, correct_cnt * 1.0 / total_cnt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
