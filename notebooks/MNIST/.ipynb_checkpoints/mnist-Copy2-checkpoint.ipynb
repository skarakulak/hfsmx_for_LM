{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "# from gensim.models import KeyedVectors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "model.fc = nn.Linear(512,10)#9*50)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318f2ea4ba0c454582ce3c906a80d2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292560f50e3d441db0a888ed69819542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a63b31fdb25465f8ee3b5cdba09ec89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a6b7fc0695404eaa449f6e251ee062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "==>>> total trainning batch number: 1875\n",
      "==>>> total testing batch number: 50\n"
     ]
    }
   ],
   "source": [
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "    \n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=200,\n",
    "                shuffle=True)\n",
    "\n",
    "print('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(test_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 1, train loss: 2.393692\n",
      "==>>> epoch: 0, batch index: 101, train loss: 0.370078\n",
      "==>>> epoch: 0, batch index: 201, train loss: 0.293662\n",
      "==>>> epoch: 0, batch index: 301, train loss: 0.140086\n",
      "==>>> epoch: 0, batch index: 401, train loss: 0.146971\n",
      "==>>> epoch: 0, batch index: 501, train loss: 0.144407\n",
      "==>>> epoch: 0, batch index: 601, train loss: 0.164758\n",
      "==>>> epoch: 0, batch index: 701, train loss: 0.112100\n",
      "==>>> epoch: 0, batch index: 801, train loss: 0.139985\n",
      "==>>> epoch: 0, batch index: 901, train loss: 0.069950\n",
      "==>>> epoch: 0, batch index: 1001, train loss: 0.063384\n",
      "==>>> epoch: 0, batch index: 1101, train loss: 0.044818\n",
      "==>>> epoch: 0, batch index: 1201, train loss: 0.096861\n",
      "==>>> epoch: 0, batch index: 1301, train loss: 0.094492\n",
      "==>>> epoch: 0, batch index: 1401, train loss: 0.064533\n",
      "==>>> epoch: 0, batch index: 1501, train loss: 0.064103\n",
      "==>>> epoch: 0, batch index: 1601, train loss: 0.066849\n",
      "==>>> epoch: 0, batch index: 1701, train loss: 0.054226\n",
      "==>>> epoch: 0, batch index: 1801, train loss: 0.072741\n",
      "==>>> epoch: 0, batch index: 1875, train loss: 0.071325\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    # trainning\n",
    "    ave_loss = None\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        x = torch.cat((x,x,x), dim=1)\n",
    "        target = target.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        ave_loss = loss.item() if ave_loss is None else ave_loss * 0.9 + loss.item() * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss))\n",
    "\n",
    "    # testing\n",
    "    correct_cnt, ave_loss_val = 0, None\n",
    "    total_cnt = 0\n",
    "    for batch_idx_val, (x, target) in enumerate(test_loader):\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        x = torch.cat((x,x,x), dim=1)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.size(0)\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        # smooth average\n",
    "        ave_loss_val = loss.item() if ave_loss_val is None else ave_loss_val * 0.9 + loss.item() * 0.1\n",
    "\n",
    "        if(batch_idx_val+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx_val+1, ave_loss_val, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.4\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.4\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.4\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.9\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.8\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.8\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.8\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.8\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.7\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.7\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.8\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.7\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.7\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.7\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.7\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.4\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.4\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.7\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.6\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.5\n",
      "Final number of leaves: 10\n",
      "Size of vocabulary: 10\n",
      "Number of internal nodes: 9\n",
      "Mean code word length: 4.4\n",
      "==>>> epoch: 0, batch index: 50, test loss: 0.053430, acc: 0.984\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x, target) in enumerate(test_loader):\n",
    "    x, target = x.to(device), target.to(device)\n",
    "    x = torch.cat((x,x,x), dim=1)\n",
    "    out = model(x)\n",
    "    loss = criterion(out, target)\n",
    "    _, pred_label = torch.max(out.data, 1)\n",
    "    total_cnt += x.data.size()[0]\n",
    "    correct_cnt += (pred_label == target.data).sum()\n",
    "    # smooth average\n",
    "    ave_loss = loss.item() if ave_loss is None else ave_loss * 0.9 + loss.item() * 0.1\n",
    "    cor_pd = pd.DataFrame.corr(pd.DataFrame(out.cpu().detach().numpy()))\n",
    "    gmm_clustering(cor_pd,f'clusters/cl_mnist{batch_idx}.txt')\n",
    "    if (batch_idx+1) == len(test_loader):\n",
    "        print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "            epoch, batch_idx+1, ave_loss, correct_cnt * 1.0 / total_cnt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_hierarchy(path):\n",
    "    tree_labels_path = {}\n",
    "    tree_label_full_path = {}\n",
    "    tree_paths = set()\n",
    "    p2t = {}\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            label, l_path = line.split(',')[:2]\n",
    "            full_path = l_path.strip()\n",
    "            p2t[full_path] = int(label)\n",
    "            tree_label_full_path[int(label)]=[int(l) for l in list(full_path)]\n",
    "            path_labels ={}\n",
    "            for k in range(1,1+len(full_path)):\n",
    "                tree_paths.add(full_path[:k-1])\n",
    "                path_labels[full_path[:k-1]] = int(full_path[k-1])\n",
    "            tree_labels_path[int(label)] = path_labels\n",
    "    path_inds = {k:i for i,k in enumerate(sorted(tree_paths,key=len))}\n",
    "    tree_labels_path_indexed = {\n",
    "        l:{path_inds[p]:p_l for p,p_l in path_dict.items()} \n",
    "        for l, path_dict in tree_labels_path.items()\n",
    "    }\n",
    "    labels_hier_idx = {}\n",
    "    for k, v in tree_labels_path_indexed.items():\n",
    "        idx,labs = list(zip(*v.items()))\n",
    "        labels_hier_idx[k] = (list(idx),list(labs))\n",
    "    return labels_hier_idx, len(tree_paths), path_inds, p2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hier_idx_l, num_of_paths_l, path_idx_l, p2t_l = [], [], [], []\n",
    "for i in range(50):\n",
    "    labels_hier_idx, num_of_paths, path_idx, p2t = get_label_hierarchy(f'clusters/cl_mnist{i}.txt')\n",
    "    labels_hier_idx = {k:(torch.tensor(v[0]).long().to(device),torch.tensor(v[1]).float().to(device)) for k,v in labels_hier_idx.items()}\n",
    "    labels_hier_idx_l.append(labels_hier_idx)\n",
    "    num_of_paths_l.append(num_of_paths)\n",
    "    path_idx_l.append(path_idx)\n",
    "    p2t_l.append(p2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HSMX = len(labels_hier_idx_l)\n",
    "NUM_PATHS = num_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_label_idx = {}\n",
    "for k in range(10):\n",
    "    comb_idx = torch.cat([h_idx[k][0] + NUM_PATHS*m for m, h_idx in enumerate(labels_hier_idx_l)])\n",
    "    comb_labels = torch.cat([h_idx[k][1] for h_idx in labels_hier_idx_l])\n",
    "    comb_label_idx[k] = (comb_idx, comb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hsmx_weights = nn.Parameter(torch.randn(512, NUM_PATHS*NUM_HSMX+1,device=device)*0.025)\n",
    "model.hsmx_bias = nn.Parameter(torch.randn(NUM_PATHS*NUM_HSMX+1,device=device)*0.025)\n",
    "def restore_zero_col():\n",
    "    model.hsmx_weights[:,-1] = 0\n",
    "    model.hsmx_bias[-1] = 0\n",
    "restore_zero_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "model.fc = Identity()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_idx = torch.cat((comb_label_idx[0][0],comb_label_idx[3][0])).unique(sorted=True)\n",
    "t_weights = model.hsmx_weights[:,used_idx]\n",
    "t_bias = model.hsmx_bias[used_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0487,  0.0009,  0.0250,  ...,  0.0268,  0.0272, -0.0083],\n",
       "        [ 0.0119, -0.0750, -0.0222,  ..., -0.0240, -0.0206,  0.0403],\n",
       "        [-0.0368,  0.0087,  0.0242,  ...,  0.0292,  0.0190,  0.0295],\n",
       "        ...,\n",
       "        [ 0.0205,  0.0034, -0.0059,  ..., -0.0117,  0.0062, -0.0273],\n",
       "        [ 0.0109, -0.0225, -0.0311,  ..., -0.0192,  0.0205, -0.0060],\n",
       "        [-0.0370,  0.0080,  0.0112,  ...,  0.0161,  0.0154, -0.0149]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(model.hsmx_weights, dim=1, torch.Tensor([0,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_ind = torch.Tensor([[1,2]]).long().expand(512,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randint(10,[3,4,5], dtype=torch.long)\n",
    "values = torch.arange(10, dtype=torch.long)\n",
    "result = torch.nonzero(tensor[..., None] == values)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4, 8, 2, 1, 6],\n",
       "         [3, 5, 2, 1, 4],\n",
       "         [0, 9, 0, 9, 5],\n",
       "         [1, 9, 6, 3, 2]],\n",
       "\n",
       "        [[5, 6, 6, 2, 2],\n",
       "         [8, 9, 2, 1, 2],\n",
       "         [3, 3, 6, 2, 2],\n",
       "         [3, 2, 2, 0, 5]],\n",
       "\n",
       "        [[7, 5, 8, 9, 0],\n",
       "         [4, 6, 8, 2, 8],\n",
       "         [5, 5, 8, 6, 3],\n",
       "         [8, 8, 8, 4, 3]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 8, 2, 1, 6, 3, 5, 2, 1, 4, 0, 9, 0, 9, 5, 1, 9, 6, 3, 2, 5, 6, 6, 2,\n",
       "        2, 8, 9, 2, 1, 2, 3, 3, 6, 2, 2, 3, 2, 2, 0, 5, 7, 5, 8, 9, 0, 4, 6, 8,\n",
       "        2, 8, 5, 5, 8, 6, 3, 8, 8, 8, 4, 3])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4, 8, 2, 1, 6],\n",
       "         [3, 5, 2, 1, 4],\n",
       "         [0, 9, 0, 9, 5],\n",
       "         [1, 9, 6, 3, 2]],\n",
       "\n",
       "        [[5, 6, 6, 2, 2],\n",
       "         [8, 9, 2, 1, 2],\n",
       "         [3, 3, 6, 2, 2],\n",
       "         [3, 2, 2, 0, 5]],\n",
       "\n",
       "        [[7, 5, 8, 9, 0],\n",
       "         [4, 6, 8, 2, 8],\n",
       "         [5, 5, 8, 6, 3],\n",
       "         [8, 8, 8, 4, 3]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 4],\n",
       "        [0, 0, 1, 8],\n",
       "        [0, 0, 2, 2],\n",
       "        [0, 0, 3, 1],\n",
       "        [0, 0, 4, 6],\n",
       "        [0, 1, 0, 3],\n",
       "        [0, 1, 1, 5],\n",
       "        [0, 1, 2, 2],\n",
       "        [0, 1, 3, 1],\n",
       "        [0, 1, 4, 4],\n",
       "        [0, 2, 0, 0],\n",
       "        [0, 2, 1, 9],\n",
       "        [0, 2, 2, 0],\n",
       "        [0, 2, 3, 9],\n",
       "        [0, 2, 4, 5],\n",
       "        [0, 3, 0, 1],\n",
       "        [0, 3, 1, 9],\n",
       "        [0, 3, 2, 6],\n",
       "        [0, 3, 3, 3],\n",
       "        [0, 3, 4, 2],\n",
       "        [1, 0, 0, 5],\n",
       "        [1, 0, 1, 6],\n",
       "        [1, 0, 2, 6],\n",
       "        [1, 0, 3, 2],\n",
       "        [1, 0, 4, 2],\n",
       "        [1, 1, 0, 8],\n",
       "        [1, 1, 1, 9],\n",
       "        [1, 1, 2, 2],\n",
       "        [1, 1, 3, 1],\n",
       "        [1, 1, 4, 2],\n",
       "        [1, 2, 0, 3],\n",
       "        [1, 2, 1, 3],\n",
       "        [1, 2, 2, 6],\n",
       "        [1, 2, 3, 2],\n",
       "        [1, 2, 4, 2],\n",
       "        [1, 3, 0, 3],\n",
       "        [1, 3, 1, 2],\n",
       "        [1, 3, 2, 2],\n",
       "        [1, 3, 3, 0],\n",
       "        [1, 3, 4, 5],\n",
       "        [2, 0, 0, 7],\n",
       "        [2, 0, 1, 5],\n",
       "        [2, 0, 2, 8],\n",
       "        [2, 0, 3, 9],\n",
       "        [2, 0, 4, 0],\n",
       "        [2, 1, 0, 4],\n",
       "        [2, 1, 1, 6],\n",
       "        [2, 1, 2, 8],\n",
       "        [2, 1, 3, 2],\n",
       "        [2, 1, 4, 8],\n",
       "        [2, 2, 0, 5],\n",
       "        [2, 2, 1, 5],\n",
       "        [2, 2, 2, 8],\n",
       "        [2, 2, 3, 6],\n",
       "        [2, 2, 4, 3],\n",
       "        [2, 3, 0, 8],\n",
       "        [2, 3, 1, 8],\n",
       "        [2, 3, 2, 8],\n",
       "        [2, 3, 3, 4],\n",
       "        [2, 3, 4, 3]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0009,  0.0250],\n",
       "        [-0.0750, -0.0222],\n",
       "        [ 0.0087,  0.0242],\n",
       "        ...,\n",
       "        [ 0.0034, -0.0059],\n",
       "        [-0.0225, -0.0311],\n",
       "        [ 0.0080,  0.0112]], device='cuda:0', grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(model.hsmx_weights,dim=1,index=gather_ind.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2],\n",
       "        [  1,   1,   2],\n",
       "        [  2,   1,   2],\n",
       "        ...,\n",
       "        [509,   1,   2],\n",
       "        [510,   1,   2],\n",
       "        [511,   1,   2]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.arange(512)[:,None], torch.Tensor([[1,2]]).long().expand(512,2)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0487,  0.0009,  0.0250],\n",
       "        [ 0.0119, -0.0750, -0.0222],\n",
       "        [-0.0368,  0.0087,  0.0242],\n",
       "        ...,\n",
       "        [ 0.0205,  0.0034, -0.0059],\n",
       "        [ 0.0109, -0.0225, -0.0311],\n",
       "        [-0.0370,  0.0080,  0.0112]], device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hsmx_weights[:,[0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_class_hsfmx(pred, path_idx, p2t, start_ind=0):\n",
    "    current_node=0\n",
    "    current_path = []\n",
    "    cur_node_path_idx = [0]\n",
    "    while True:     \n",
    "        next_path_pred = pred[start_ind+cur_node_path_idx[-1]]\n",
    "        current_path.append('1' if next_path_pred.item() >= 0 else '0')\n",
    "        new_path = ''.join(current_path)\n",
    "        if new_path in p2t:\n",
    "            return p2t[new_path]\n",
    "        cur_node_path_idx.append(path_idx[new_path])\n",
    "\n",
    "def pred_batch(output, path_idx_l, p2t_l):\n",
    "    return torch.Tensor([\n",
    "        pred_class_hsfmx(row, path_idx, p2t, k*NUM_PATHS) \n",
    "        for row in output\n",
    "        for k, (path_idx, p2t) in enumerate(zip(path_idx_l,p2t_l))\n",
    "    ]).long().to(device).reshape(output.size(0), NUM_HSMX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([450])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "criterion_hsmx = nn.BCEWithLogitsLoss()\n",
    "for epoch in range(10):\n",
    "    # trainning\n",
    "    ave_loss = None\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        x = torch.cat((x,x,x), dim=1)\n",
    "        target = target.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        y_hsmx_idx = torch.cat([row*NUM_PATHS*NUM_HSMX + comb_label_idx[l][0]  for row, l in enumerate(target.tolist())])\n",
    "        y_hsmx_labels = torch.cat([comb_label_idx[l][1]  for row, l in enumerate(target.tolist())])\n",
    "        out_hsmx =  torch.gather(out.flatten(), 0, y_hsmx_idx)\n",
    "        loss  = criterion_hsmx(out_hsmx,y_hsmx_labels)\n",
    "\n",
    "        ave_loss = loss.item() if ave_loss is None else ave_loss * 0.9 + loss.item() * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss))\n",
    "            \n",
    "    # testing\n",
    "    correct_cnt, ave_loss_val = 0, None\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        x = torch.cat((x,x,x), dim=1)\n",
    "        out = model(x)\n",
    "        y_hsmx_idx = torch.cat([row*num_of_paths + comb_label_idx[l][0]  for row, l in enumerate(target.tolist())])\n",
    "        y_hsmx_labels = torch.cat([comb_label_idx[l][1]  for row, l in enumerate(target.tolist())])\n",
    "        out_hsmx =  torch.gather(out.flatten(), 0, y_hsmx_idx)\n",
    "        loss  = criterion_hsmx(out_hsmx,y_hsmx_labels)\n",
    "\n",
    "        pred_label = pred_batch(out, path_idx_l, p2t_l)\n",
    "        pred_label_mode = pred_label.mode(dim=1)[0]\n",
    "        total_cnt += x.size(0)\n",
    "        correct_cnt += (pred_label_mode == target).sum()\n",
    "        # smooth average\n",
    "        ave_loss_val = loss.item() if ave_loss_val is None else ave_loss_val * 0.9 + loss.item() * 0.1\n",
    "\n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss_val, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 50, test loss: 0.759030, acc: 0.597\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "correct_cnt, ave_loss_val = 0, None\n",
    "total_cnt = 0\n",
    "for batch_idx, (x, target) in enumerate(test_loader):\n",
    "    x, target = x.to(device), target.to(device)\n",
    "    x = torch.cat((x,x,x), dim=1)\n",
    "    out = model(x)\n",
    "    y_hsmx_idx = torch.cat([row*num_of_paths + comb_label_idx[l][0]  for row, l in enumerate(target.tolist())])\n",
    "    y_hsmx_labels = torch.cat([comb_label_idx[l][1]  for row, l in enumerate(target.tolist())])\n",
    "    out_hsmx =  torch.gather(out.flatten(), 0, y_hsmx_idx)\n",
    "    loss  = criterion_hsmx(out_hsmx,y_hsmx_labels)\n",
    "\n",
    "    pred_label = pred_batch(out, path_idx_l, p2t_l)\n",
    "    pred_label_mode = pred_label.mode(dim=1)[0]\n",
    "    total_cnt += x.size(0)\n",
    "    correct_cnt += (pred_label_mode == target).sum()\n",
    "    # smooth average\n",
    "    ave_loss_val = loss.item() if ave_loss_val is None else ave_loss_val * 0.9 + loss.item() * 0.1\n",
    "\n",
    "    if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "        print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "            epoch, batch_idx+1, ave_loss_val, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = pred_batch(out, path_idx_l, p2t_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([10])\n",
      "1 torch.Size([10])\n",
      "2 torch.Size([10])\n",
      "3 torch.Size([9])\n",
      "4 torch.Size([10])\n",
      "5 torch.Size([8])\n",
      "6 torch.Size([10])\n",
      "7 torch.Size([9])\n",
      "8 torch.Size([10])\n",
      "9 torch.Size([10])\n",
      "10 torch.Size([10])\n",
      "11 torch.Size([10])\n",
      "12 torch.Size([10])\n",
      "13 torch.Size([10])\n",
      "14 torch.Size([10])\n",
      "15 torch.Size([10])\n",
      "16 torch.Size([10])\n",
      "17 torch.Size([10])\n",
      "18 torch.Size([10])\n",
      "19 torch.Size([10])\n",
      "20 torch.Size([9])\n",
      "21 torch.Size([9])\n",
      "22 torch.Size([10])\n",
      "23 torch.Size([10])\n",
      "24 torch.Size([10])\n",
      "25 torch.Size([10])\n",
      "26 torch.Size([10])\n",
      "27 torch.Size([10])\n",
      "28 torch.Size([10])\n",
      "29 torch.Size([10])\n",
      "30 torch.Size([10])\n",
      "31 torch.Size([10])\n",
      "32 torch.Size([10])\n",
      "33 torch.Size([10])\n",
      "34 torch.Size([10])\n",
      "35 torch.Size([9])\n",
      "36 torch.Size([10])\n",
      "37 torch.Size([10])\n",
      "38 torch.Size([10])\n",
      "39 torch.Size([10])\n",
      "40 torch.Size([10])\n",
      "41 torch.Size([10])\n",
      "42 torch.Size([10])\n",
      "43 torch.Size([10])\n",
      "44 torch.Size([10])\n",
      "45 torch.Size([10])\n",
      "46 torch.Size([10])\n",
      "47 torch.Size([9])\n",
      "48 torch.Size([10])\n",
      "49 torch.Size([10])\n",
      "50 torch.Size([10])\n",
      "51 torch.Size([10])\n",
      "52 torch.Size([10])\n",
      "53 torch.Size([10])\n",
      "54 torch.Size([9])\n",
      "55 torch.Size([10])\n",
      "56 torch.Size([10])\n",
      "57 torch.Size([10])\n",
      "58 torch.Size([10])\n",
      "59 torch.Size([10])\n",
      "60 torch.Size([10])\n",
      "61 torch.Size([10])\n",
      "62 torch.Size([10])\n",
      "63 torch.Size([10])\n",
      "64 torch.Size([10])\n",
      "65 torch.Size([9])\n",
      "66 torch.Size([10])\n",
      "67 torch.Size([10])\n",
      "68 torch.Size([10])\n",
      "69 torch.Size([10])\n",
      "70 torch.Size([10])\n",
      "71 torch.Size([10])\n",
      "72 torch.Size([10])\n",
      "73 torch.Size([10])\n",
      "74 torch.Size([9])\n",
      "75 torch.Size([10])\n",
      "76 torch.Size([10])\n",
      "77 torch.Size([10])\n",
      "78 torch.Size([10])\n",
      "79 torch.Size([10])\n",
      "80 torch.Size([10])\n",
      "81 torch.Size([10])\n",
      "82 torch.Size([9])\n",
      "83 torch.Size([10])\n",
      "84 torch.Size([10])\n",
      "85 torch.Size([10])\n",
      "86 torch.Size([10])\n",
      "87 torch.Size([10])\n",
      "88 torch.Size([10])\n",
      "89 torch.Size([10])\n",
      "90 torch.Size([10])\n",
      "91 torch.Size([10])\n",
      "92 torch.Size([10])\n",
      "93 torch.Size([10])\n",
      "94 torch.Size([9])\n",
      "95 torch.Size([10])\n",
      "96 torch.Size([9])\n",
      "97 torch.Size([10])\n",
      "98 torch.Size([10])\n",
      "99 torch.Size([10])\n",
      "100 torch.Size([9])\n",
      "101 torch.Size([10])\n",
      "102 torch.Size([10])\n",
      "103 torch.Size([10])\n",
      "104 torch.Size([10])\n",
      "105 torch.Size([10])\n",
      "106 torch.Size([9])\n",
      "107 torch.Size([10])\n",
      "108 torch.Size([10])\n",
      "109 torch.Size([10])\n",
      "110 torch.Size([10])\n",
      "111 torch.Size([9])\n",
      "112 torch.Size([10])\n",
      "113 torch.Size([10])\n",
      "114 torch.Size([10])\n",
      "115 torch.Size([10])\n",
      "116 torch.Size([10])\n",
      "117 torch.Size([10])\n",
      "118 torch.Size([9])\n",
      "119 torch.Size([10])\n",
      "120 torch.Size([10])\n",
      "121 torch.Size([10])\n",
      "122 torch.Size([10])\n",
      "123 torch.Size([10])\n",
      "124 torch.Size([10])\n",
      "125 torch.Size([10])\n",
      "126 torch.Size([10])\n",
      "127 torch.Size([10])\n",
      "128 torch.Size([10])\n",
      "129 torch.Size([10])\n",
      "130 torch.Size([10])\n",
      "131 torch.Size([10])\n",
      "132 torch.Size([10])\n",
      "133 torch.Size([10])\n",
      "134 torch.Size([10])\n",
      "135 torch.Size([9])\n",
      "136 torch.Size([10])\n",
      "137 torch.Size([10])\n",
      "138 torch.Size([10])\n",
      "139 torch.Size([10])\n",
      "140 torch.Size([10])\n",
      "141 torch.Size([10])\n",
      "142 torch.Size([10])\n",
      "143 torch.Size([10])\n",
      "144 torch.Size([10])\n",
      "145 torch.Size([10])\n",
      "146 torch.Size([10])\n",
      "147 torch.Size([10])\n",
      "148 torch.Size([10])\n",
      "149 torch.Size([10])\n",
      "150 torch.Size([10])\n",
      "151 torch.Size([10])\n",
      "152 torch.Size([10])\n",
      "153 torch.Size([10])\n",
      "154 torch.Size([10])\n",
      "155 torch.Size([10])\n",
      "156 torch.Size([9])\n",
      "157 torch.Size([10])\n",
      "158 torch.Size([10])\n",
      "159 torch.Size([10])\n",
      "160 torch.Size([10])\n",
      "161 torch.Size([10])\n",
      "162 torch.Size([9])\n",
      "163 torch.Size([9])\n",
      "164 torch.Size([10])\n",
      "165 torch.Size([10])\n",
      "166 torch.Size([10])\n",
      "167 torch.Size([10])\n",
      "168 torch.Size([9])\n",
      "169 torch.Size([10])\n",
      "170 torch.Size([10])\n",
      "171 torch.Size([10])\n",
      "172 torch.Size([10])\n",
      "173 torch.Size([9])\n",
      "174 torch.Size([10])\n",
      "175 torch.Size([9])\n",
      "176 torch.Size([10])\n",
      "177 torch.Size([10])\n",
      "178 torch.Size([10])\n",
      "179 torch.Size([10])\n",
      "180 torch.Size([10])\n",
      "181 torch.Size([10])\n",
      "182 torch.Size([10])\n",
      "183 torch.Size([10])\n",
      "184 torch.Size([10])\n",
      "185 torch.Size([10])\n",
      "186 torch.Size([10])\n",
      "187 torch.Size([10])\n",
      "188 torch.Size([10])\n",
      "189 torch.Size([10])\n",
      "190 torch.Size([10])\n",
      "191 torch.Size([10])\n",
      "192 torch.Size([10])\n",
      "193 torch.Size([10])\n",
      "194 torch.Size([10])\n",
      "195 torch.Size([10])\n",
      "196 torch.Size([10])\n",
      "197 torch.Size([10])\n",
      "198 torch.Size([9])\n",
      "199 torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(pred_label):\n",
    "    print(i, row.unique().size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
